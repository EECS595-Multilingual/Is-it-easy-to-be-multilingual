{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, DefaultDataCollator, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import collections\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_lang2file = {\n",
    "    'en' : 'tydiqa.en.train.json',\n",
    "    'fi' : 'tydiqa.fi.train.json',\n",
    "    'ar' : 'tydiqa.ar.train.json',\n",
    "    'bn' : 'tydiqa.bn.train.json',\n",
    "    'id' : 'tydiqa.in.train.json',\n",
    "    'ko' : 'tydiqa.ko.train.json',\n",
    "    'ru' : 'tydiqa.ru.train.json',\n",
    "    'sw' : 'tydiqa.sw.train.json',\n",
    "    'te' : 'tydiqa.te.train.json',\n",
    "}\n",
    "\n",
    "T_lang2file = {\n",
    "    'en' : 'tydiqa.en.dev.json',\n",
    "    'fi' : 'tydiqa.fi.dev.json',\n",
    "    'ar' : 'tydiqa.ar.dev.json',\n",
    "    'bn' : 'tydiqa.bn.dev.json',\n",
    "    'id' : 'tydiqa.in.dev.json',\n",
    "    'ko' : 'tydiqa.ko.dev.json',\n",
    "    'ru' : 'tydiqa.ru.dev.json',\n",
    "    'sw' : 'tydiqa.sw.dev.json',\n",
    "    'te' : 'tydiqa.te.dev.json',\n",
    "}\n",
    "\n",
    "accuracy_dict = {} # for storing all the test accuracies in the form { (S,T,SHOT) , Acc }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/rishikesh/Desktop/Project/download/tydiqa/\"\n",
    "\n",
    "def read_data(path):  \n",
    "    with open(path, 'rb') as f:\n",
    "        squad = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    id = []\n",
    "\n",
    "    for group in squad['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "                    id.append(qa['id'])\n",
    "\n",
    "    return contexts, questions, answers, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Languages: en, fi, ar, bn, id, ko, ru, sw, te = 9\n",
    "# Total Language pairs = 9*9 = 81\n",
    "\n",
    "SHOT = 0 # 0-shot or few-shot\n",
    "\n",
    "S = 'en'\n",
    "T = 'en'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_path = path + 'tydiqa-goldp-v1.1-train/' + S_lang2file[S]\n",
    "t_path = path + 'tydiqa-goldp-v1.1-dev/' + T_lang2file[T]\n",
    "\n",
    "s_context, s_q, s_a, s_i = read_data(s_path)\n",
    "t_context, t_q, t_a, t_i = read_data(t_path)\n",
    "\n",
    "if SHOT>0:\n",
    "    few_shot_path = path + 'tydiqa-goldp-v1.1-train/' + S_lang2file[T]\n",
    "    fs_context, fs_q, fs_a, fs_i = read_data(few_shot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_tydi = []\n",
    "for _ in range(len(s_a)):\n",
    "    s_tydi.append({})\n",
    "    s_tydi[_]['answers'] = s_a[_]\n",
    "    s_tydi[_]['context'] = s_context[_]\n",
    "    s_tydi[_]['question'] = s_q[_]\n",
    "    s_tydi[_]['id'] = s_i[_]\n",
    "\n",
    "    \n",
    "if SHOT>0:\n",
    "    for _ in range(SHOT):\n",
    "        s_tydi.append({})\n",
    "        s_tydi[len(s_tydi) - 1]['answers'] = fs_a[_]\n",
    "        s_tydi[len(s_tydi) - 1]['context'] = fs_context[_]\n",
    "        s_tydi[len(s_tydi) - 1]['question'] = fs_q[_]\n",
    "        s_tydi[len(s_tydi) - 1]['id'] = fs_i[_]\n",
    "\n",
    "s_data = Dataset.from_list(s_tydi)\n",
    "\n",
    "t_tydi = []\n",
    "for _ in range(len(t_a)):\n",
    "    t_tydi.append({})\n",
    "    t_tydi[_]['answers'] = t_a[_]\n",
    "    t_tydi[_]['context'] = t_context[_]\n",
    "    t_tydi[_]['question'] = t_q[_] \n",
    "    t_tydi[_]['id'] = t_i[_] \n",
    "t_data = Dataset.from_list(t_tydi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_function(examples):\n",
    "#     questions = [q.strip() for q in examples[\"question\"]]\n",
    "#     inputs = tokenizer(\n",
    "#         questions,\n",
    "#         examples[\"context\"],\n",
    "#         max_length=400,\n",
    "#         truncation=\"only_second\",\n",
    "#         return_overflowing_tokens=True,\n",
    "#         return_offsets_mapping=True,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "\n",
    "#     sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "#     offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "#     inputs[\"start_positions\"] = []\n",
    "#     inputs[\"end_positions\"] = []\n",
    "\n",
    "#     for i, offset in enumerate(offset_mapping):\n",
    "#         input_ids = inputs[\"input_ids\"][i]\n",
    "#         cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "#         sequence_ids = inputs.sequence_ids(i)\n",
    "#         sample_index = sample_mapping[i]\n",
    "#         answers = examples['answers'][sample_index]\n",
    "        \n",
    "#         if answers[\"answer_start\"] == '':\n",
    "#             inputs[\"start_positions\"].append(cls_index)\n",
    "#             inputs[\"end_positions\"].append(cls_index)\n",
    "#         else:\n",
    "#             # Start/end character index of the answer in the text.\n",
    "#             start_char = answers[\"answer_start\"]\n",
    "#             end_char = start_char + len(answers[\"text\"])\n",
    "\n",
    "#             # Start token index of the current span in the text.\n",
    "#             token_start_index = 0\n",
    "#             while sequence_ids[token_start_index] != 1:\n",
    "#                 token_start_index += 1\n",
    "\n",
    "#             # End token index of the current span in the text.\n",
    "#             token_end_index = len(input_ids) - 1\n",
    "#             while sequence_ids[token_end_index] != 1:\n",
    "#                 token_end_index -= 1\n",
    "\n",
    "#             # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "#             if not (offset[token_start_index][0] <= start_char and offset[token_end_index][1] >= end_char):\n",
    "#                 inputs[\"start_positions\"].append(cls_index)\n",
    "#                 inputs[\"end_positions\"].append(cls_index)\n",
    "#             else:\n",
    "#                 # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "#                 # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "#                 while token_start_index < len(offset) and offset[token_start_index][0] <= start_char:\n",
    "#                     token_start_index += 1\n",
    "#                 inputs[\"start_positions\"].append(token_start_index - 1)\n",
    "#                 while offset[token_end_index][1] >= end_char:\n",
    "#                     token_end_index -= 1\n",
    "#                 inputs[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "#     return inputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"]\n",
    "        end_char = answer[\"answer_start\"] + len(answer[\"text\"])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ac483ad2ab4a59a5a5702e1ba4315b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3696 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3696, 3804)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = s_data.map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=s_data.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_validation_features(examples):\n",
    "#     # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "#     # in one example possible giving several features when a context is long, each of those features having a\n",
    "#     # context that overlaps a bit the context of the previous feature.\n",
    "#     tokenized_examples = tokenizer(\n",
    "#         examples[\"question\"],\n",
    "#         examples[\"context\"],\n",
    "#         truncation=\"only_second\",\n",
    "#         max_length=400,\n",
    "#         return_overflowing_tokens=True,\n",
    "#         return_offsets_mapping=True,\n",
    "#         padding=\"max_length\"\n",
    "#     )\n",
    "\n",
    "#     # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "#     # its corresponding example. This key gives us just that.\n",
    "#     sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "#     # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
    "#     # corresponding example_id and we will store the offset mappings.\n",
    "\n",
    "#     tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "#     for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "#         # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "#         sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "#         context_index = 1\n",
    "\n",
    "#         # One example can give several spans, this is the index of the example containing this span of text.\n",
    "#         sample_index = sample_mapping[i]\n",
    "#         tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "#         # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "#         # position is part of the context or not.\n",
    "#         tokenized_examples[\"offset_mapping\"][i] = [\n",
    "#             (o if sequence_ids[k] == context_index else None)\n",
    "#             for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "#         ]\n",
    "\n",
    "#     return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ecd1069fac24722ab78557762485764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(440, 447)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = t_data.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=t_data.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_s_data = s_data.map(preprocess_function, batched=True, batch_size=32, remove_columns=s_data.column_names)\n",
    "# tokenized_t_data = t_data.map(prepare_validation_features, batched=True, batch_size=32, remove_columns=t_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -20 - 1 : -1].tolist() #n_best\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -20 - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > 60 #max_answer_len\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": {\"text\":[ex[\"answers\"][\"text\"]], \"answer_start\":[ex[\"answers\"][\"answer_start\"]]}} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='476' max='476' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [476/476 52:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=476, training_loss=1.9688453994879203, metrics={'train_runtime': 3180.9311, 'train_samples_per_second': 1.196, 'train_steps_per_second': 0.15, 'total_flos': 745479646967808.0, 'train_loss': 1.9688453994879203, 'epoch': 1.0})"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DefaultDataCollator()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='OP',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4b20632a0143b5811cff3e346bdf6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "f1 = compute_metrics(start_logits, end_logits, validation_dataset, t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.84189610164601"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict[(S,T,SHOT)] = f1['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|█████████████████████| 116/116 [53:26<00:00, 27.65s/it, loss=5.93]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Average Loss: 5.92772757595983, Training Accuracy: 0.013392857142857142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # TRAIN\n",
    "# tokenized_s_data.set_format(\"torch\")\n",
    "# train_dataloader = DataLoader(tokenized_s_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     total_correct = 0\n",
    "#     total_samples = 0\n",
    "    \n",
    "#     loop = tqdm(train_dataloader, leave=True)\n",
    "#     for batch in loop:\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         start_positions = batch[\"start_positions\"].to(device)\n",
    "#         end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             start_positions=start_positions,\n",
    "#             end_positions=end_positions\n",
    "#         )\n",
    "\n",
    "#         loss = outputs.loss\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Get predicted start and end positions\n",
    "#         pred_start_positions = torch.argmax(outputs.start_logits, dim=1)\n",
    "#         pred_end_positions = torch.argmax(outputs.end_logits, dim=1)\n",
    "\n",
    "#         # Calculate accuracy\n",
    "#         correct_start = (pred_start_positions == start_positions).sum().item()\n",
    "#         correct_end = (pred_end_positions == end_positions).sum().item()\n",
    "\n",
    "#         total_correct += correct_start + correct_end\n",
    "#         total_samples += start_positions.size(0) * 2  # Multiply by 2 as we have start and end positions\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         loop.set_description(f'Epoch {epoch+1}')\n",
    "#         loop.set_postfix(loss=loss.item())\n",
    "\n",
    "#     average_loss = total_loss / len(train_dataloader)\n",
    "#     accuracy = total_correct / total_samples\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss}, Training Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [02:26<00:00,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.0159846547314578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # TEST\n",
    "# tokenized_t_data.set_format(\"torch\")\n",
    "# test_dataloader = DataLoader(tokenized_t_data, batch_size=32)\n",
    "\n",
    "# model.eval()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# total_correct = 0\n",
    "# total_samples = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in tqdm(test_dataloader):\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         start_positions = batch[\"start_positions\"].to(device)\n",
    "#         end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "#         outputs = model(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             start_positions=start_positions,\n",
    "#             end_positions=end_positions\n",
    "#         )\n",
    "\n",
    "#         # Get predicted start and end positions\n",
    "#         pred_start_positions = torch.argmax(outputs.start_logits, dim=1)\n",
    "#         pred_end_positions = torch.argmax(outputs.end_logits, dim=1)\n",
    "\n",
    "#         # Calculate accuracy\n",
    "#         correct_start = (pred_start_positions == start_positions).sum().item()\n",
    "#         correct_end = (pred_end_positions == end_positions).sum().item()\n",
    "\n",
    "#         total_correct += correct_start + correct_end\n",
    "#         total_samples += start_positions.size(0) * 2  # Multiply by 2 as we have start and end positions\n",
    "\n",
    "# accuracy = total_correct / total_samples\n",
    "# print(f\"Testing Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
