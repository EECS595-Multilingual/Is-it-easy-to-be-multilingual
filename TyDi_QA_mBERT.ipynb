{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, DefaultDataCollator, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_lang2file = {\n",
    "    'en' : 'tydiqa.en.train.json',\n",
    "    'fi' : 'tydiqa.fi.train.json',\n",
    "    'ar' : 'tydiqa.ar.train.json',\n",
    "    'bn' : 'tydiqa.bn.train.json',\n",
    "    'id' : 'tydiqa.in.train.json',\n",
    "    'ko' : 'tydiqa.ko.train.json',\n",
    "    'ru' : 'tydiqa.ru.train.json',\n",
    "    'sw' : 'tydiqa.sw.train.json',\n",
    "    'te' : 'tydiqa.te.train.json',\n",
    "}\n",
    "\n",
    "T_lang2file = {\n",
    "    'en' : 'tydiqa.en.dev.json',\n",
    "    'fi' : 'tydiqa.fi.dev.json',\n",
    "    'ar' : 'tydiqa.ar.dev.json',\n",
    "    'bn' : 'tydiqa.bn.dev.json',\n",
    "    'id' : 'tydiqa.in.dev.json',\n",
    "    'ko' : 'tydiqa.ko.dev.json',\n",
    "    'ru' : 'tydiqa.ru.dev.json',\n",
    "    'sw' : 'tydiqa.sw.dev.json',\n",
    "    'te' : 'tydiqa.te.dev.json',\n",
    "}\n",
    "\n",
    "accuracy_dict = {} # for storing all the test accuracies in the form { (S,T,SHOT) , Acc }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/rishikesh/Desktop/Project/download/tydiqa/\"\n",
    "\n",
    "def read_data(path):  \n",
    "    with open(path, 'rb') as f:\n",
    "        squad = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    for group in squad['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Languages: en, fi, ar, bn, id, ko, ru, sw, te = 9\n",
    "# Total Language pairs = 9*9 = 81\n",
    "\n",
    "SHOT = 0 # 0-shot or few-shot\n",
    "\n",
    "S = 'en'\n",
    "T = 'fi'\n",
    "\n",
    "#### To run it all at once  -\n",
    "# for S in S_lang2file.keys():\n",
    "#     for T in T_lang2file.keys():\n",
    "        ### All the code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_path = path + 'tydiqa-goldp-v1.1-train/' + S_lang2file[S]\n",
    "t_path = path + 'tydiqa-goldp-v1.1-dev/' + T_lang2file[T]\n",
    "\n",
    "s_context, s_q, s_a = read_data(s_path)\n",
    "t_context, t_q, t_a = read_data(t_path)\n",
    "\n",
    "if SHOT>0:\n",
    "    few_shot_path = path + 'tydiqa-goldp-v1.1-train/' + S_lang2file[T]\n",
    "    fs_context, fs_q, fs_a = read_data(few_shot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_tydi = []\n",
    "for _ in range(len(s_a)):\n",
    "    s_tydi.append({})\n",
    "    s_tydi[_]['answers'] = s_a[_]\n",
    "    s_tydi[_]['context'] = s_context[_]\n",
    "    s_tydi[_]['question'] = s_q[_]\n",
    "\n",
    "    \n",
    "if SHOT>0:\n",
    "    for _ in range(SHOT):\n",
    "        s_tydi.append({})\n",
    "        s_tydi[len(s_tydi) - 1]['answers'] = fs_a[_]\n",
    "        s_tydi[len(s_tydi) - 1]['context'] = fs_context[_]\n",
    "        s_tydi[len(s_tydi) - 1]['question'] = fs_q[_]\n",
    "\n",
    "s_data = Dataset.from_list(s_tydi)\n",
    "\n",
    "t_tydi = []\n",
    "for _ in range(len(t_a)):\n",
    "    t_tydi.append({})\n",
    "    t_tydi[_]['answers'] = t_a[_]\n",
    "    t_tydi[_]['context'] = t_context[_]\n",
    "    t_tydi[_]['question'] = t_q[_]  \n",
    "t_data = Dataset.from_list(t_tydi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=400,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"]\n",
    "        end_char = answer[\"answer_start\"] + len(answer[\"text\"])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] > end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec197dfc62649d680af2bf16c5e1efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3696 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac73f2a703447b3934150acaf7edabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/782 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_s_data = s_data.map(preprocess_function, batched=True, batch_size=32, remove_columns=s_data.column_names)\n",
    "tokenized_t_data = t_data.map(preprocess_function, batched=True, batch_size=32, remove_columns=t_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = DefaultDataCollator()\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='OP',\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     num_train_epochs=1,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_s_data,\n",
    "#     eval_dataset=tokenized_t_data,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|█████████████████████| 116/116 [53:26<00:00, 27.65s/it, loss=5.93]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Average Loss: 5.92772757595983, Training Accuracy: 0.013392857142857142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "tokenized_s_data.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(tokenized_s_data, batch_size=32, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        start_positions = batch[\"start_positions\"].to(device)\n",
    "        end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            start_positions=start_positions,\n",
    "            end_positions=end_positions\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Get predicted start and end positions\n",
    "        pred_start_positions = torch.argmax(outputs.start_logits, dim=1)\n",
    "        pred_end_positions = torch.argmax(outputs.end_logits, dim=1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct_start = (pred_start_positions == start_positions).sum().item()\n",
    "        correct_end = (pred_end_positions == end_positions).sum().item()\n",
    "\n",
    "        total_correct += correct_start + correct_end\n",
    "        total_samples += start_positions.size(0) * 2  # Multiply by 2 as we have start and end positions\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch+1}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss}, Training Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [02:26<00:00,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.0159846547314578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "tokenized_t_data.set_format(\"torch\")\n",
    "test_dataloader = DataLoader(tokenized_t_data, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        start_positions = batch[\"start_positions\"].to(device)\n",
    "        end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            start_positions=start_positions,\n",
    "            end_positions=end_positions\n",
    "        )\n",
    "\n",
    "        # Get predicted start and end positions\n",
    "        pred_start_positions = torch.argmax(outputs.start_logits, dim=1)\n",
    "        pred_end_positions = torch.argmax(outputs.end_logits, dim=1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct_start = (pred_start_positions == start_positions).sum().item()\n",
    "        correct_end = (pred_end_positions == end_positions).sum().item()\n",
    "\n",
    "        total_correct += correct_start + correct_end\n",
    "        total_samples += start_positions.size(0) * 2  # Multiply by 2 as we have start and end positions\n",
    "\n",
    "accuracy = total_correct / total_samples\n",
    "print(f\"Testing Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict[(S,T,SHOT)] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('en', 'en', 0): 0.01818181818181818, ('en', 'fi', 0): 0.0159846547314578}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
